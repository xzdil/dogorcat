{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMyQxrGFmnWHDzrnTlA1/6v",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/xzdil/googlecollabprojects/blob/main/Custom_LLM_From_GradioAPI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio_client"
      ],
      "metadata": {
        "id": "GQJ14Am-eE63"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "MgxENBFed6A7"
      },
      "outputs": [],
      "source": [
        "from gradio_client import Client"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "client = Client(\"https://72e592a6e793c00cd5.gradio.live/\")  # connecting to a temporary Gradio share URL"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q0yRubDYd92f",
        "outputId": "795c0327-3fac-4156-ff6b-7c2077365bb8"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded as API: https://72e592a6e793c00cd5.gradio.live/ ✔\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "job = client.submit(\"Как пожарить омлет\", api_name=\"/chat\")  # runs the prediction in a background thread\n",
        "for i in job:\n",
        "  print(i, end='')"
      ],
      "metadata": {
        "id": "c373qV4feciU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install llama-index"
      ],
      "metadata": {
        "id": "zPABPcvKmcL6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Any, Callable, Dict, Optional, Sequence\n",
        "\n",
        "import requests\n",
        "from llama_index.core.base.llms.types import (\n",
        "    ChatMessage,\n",
        "    ChatResponse,\n",
        "    ChatResponseGen,\n",
        "    CompletionResponse,\n",
        "    CompletionResponseGen,\n",
        "    LLMMetadata,\n",
        ")\n",
        "from llama_index.core.bridge.pydantic import Field, PrivateAttr\n",
        "from llama_index.core.callbacks import CallbackManager\n",
        "from llama_index.core.constants import (\n",
        "    DEFAULT_CONTEXT_WINDOW,\n",
        "    DEFAULT_NUM_OUTPUTS,\n",
        "    DEFAULT_TEMPERATURE,\n",
        ")\n",
        "from llama_index.core.llms.callbacks import llm_chat_callback, llm_completion_callback\n",
        "from llama_index.core.llms.custom import CustomLLM\n",
        "from llama_index.core.base.llms.generic_utils import (\n",
        "    completion_response_to_chat_response,\n",
        "    stream_completion_response_to_chat_response,\n",
        ")\n",
        "from llama_index.core.types import BaseOutputParser, PydanticProgramMode\n",
        "from llama_index.core.utils import get_cache_dir\n",
        "from tqdm import tqdm\n",
        "\n",
        "DEFAULT_LLAMA_CPP_MODEL_VERBOSITY = False\n",
        "class LlamaCPP(CustomLLM):\n",
        "    model_url: Optional[str] = Field(\n",
        "        description=\"The URL llama-cpp model to download and use.\"\n",
        "    )\n",
        "    model_path: Optional[str] = Field(\n",
        "        description=\"The path to the llama-cpp model to use.\"\n",
        "    )\n",
        "    temperature: float = Field(\n",
        "        default=DEFAULT_TEMPERATURE,\n",
        "        description=\"The temperature to use for sampling.\",\n",
        "        gte=0.0,\n",
        "        lte=1.0,\n",
        "    )\n",
        "    max_new_tokens: int = Field(\n",
        "        default=DEFAULT_NUM_OUTPUTS,\n",
        "        description=\"The maximum number of tokens to generate.\",\n",
        "        gt=0,\n",
        "    )\n",
        "    context_window: int = Field(\n",
        "        default=DEFAULT_CONTEXT_WINDOW,\n",
        "        description=\"The maximum number of context tokens for the model.\",\n",
        "        gt=0,\n",
        "    )\n",
        "    generate_kwargs: Dict[str, Any] = Field(\n",
        "        default_factory=dict, description=\"Kwargs used for generation.\"\n",
        "    )\n",
        "    model_kwargs: Dict[str, Any] = Field(\n",
        "        default_factory=dict, description=\"Kwargs used for model initialization.\"\n",
        "    )\n",
        "    verbose: bool = Field(\n",
        "        default=DEFAULT_LLAMA_CPP_MODEL_VERBOSITY,\n",
        "        description=\"Whether to print verbose output.\",\n",
        "    )\n",
        "\n",
        "    _model: Any = PrivateAttr()\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        model_url: Optional[str] = None,\n",
        "        model_path: Optional[str] = None,\n",
        "        temperature: float = DEFAULT_TEMPERATURE,\n",
        "        max_new_tokens: int = DEFAULT_NUM_OUTPUTS,\n",
        "        context_window: int = DEFAULT_CONTEXT_WINDOW,\n",
        "        callback_manager: Optional[CallbackManager] = None,\n",
        "        generate_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        model_kwargs: Optional[Dict[str, Any]] = None,\n",
        "        verbose: bool = DEFAULT_LLAMA_CPP_MODEL_VERBOSITY,\n",
        "        system_prompt: Optional[str] = None,\n",
        "        messages_to_prompt: Optional[Callable[[Sequence[ChatMessage]], str]] = None,\n",
        "        completion_to_prompt: Optional[Callable[[str], str]] = None,\n",
        "        pydantic_program_mode: PydanticProgramMode = PydanticProgramMode.DEFAULT,\n",
        "        output_parser: Optional[BaseOutputParser] = None,\n",
        "    ) -> None:\n",
        "        model_kwargs = {\n",
        "            **{\"n_ctx\": context_window, \"verbose\": verbose},\n",
        "            **(model_kwargs or {}),  # Override defaults via model_kwargs\n",
        "        }\n",
        "\n",
        "        # check if model is cached\n",
        "        if model_path is not None:\n",
        "            self._model = Client(model_path)\n",
        "        else:\n",
        "            self._model = Client(model_path)\n",
        "\n",
        "        model_path = model_path\n",
        "        generate_kwargs = generate_kwargs or {}\n",
        "        generate_kwargs.update(\n",
        "            {\"temperature\": temperature, \"max_tokens\": max_new_tokens}\n",
        "        )\n",
        "\n",
        "        super().__init__(\n",
        "            model_path=model_path,\n",
        "            model_url=model_url,\n",
        "            temperature=temperature,\n",
        "            context_window=context_window,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            callback_manager=callback_manager,\n",
        "            generate_kwargs=generate_kwargs,\n",
        "            model_kwargs=model_kwargs,\n",
        "            verbose=verbose,\n",
        "            system_prompt=system_prompt,\n",
        "            messages_to_prompt=messages_to_prompt,\n",
        "            completion_to_prompt=completion_to_prompt,\n",
        "            pydantic_program_mode=pydantic_program_mode,\n",
        "            output_parser=output_parser,\n",
        "        )\n",
        "\n",
        "    @classmethod\n",
        "    def class_name(cls) -> str:\n",
        "        return \"GradioAPI_LLM\"\n",
        "\n",
        "    @property\n",
        "    def metadata(self) -> LLMMetadata:\n",
        "        \"\"\"LLM metadata.\"\"\"\n",
        "        return LLMMetadata(\n",
        "            context_window=self._model.context_params.n_ctx,\n",
        "            num_output=self.max_new_tokens,\n",
        "            model_name=self.model_path,\n",
        "        )\n",
        "\n",
        "    def _get_model_path_for_version(self) -> str:\n",
        "        \"\"\"Get model path for the current llama-cpp version.\"\"\"\n",
        "        return \"Something gotta be here...\"\n",
        "\n",
        "    @llm_chat_callback()\n",
        "    def chat(self, messages: Sequence[ChatMessage], **kwargs: Any) -> ChatResponse:\n",
        "        prompt = self.messages_to_prompt(messages)\n",
        "        completion_response = self.complete(prompt, formatted=True, **kwargs)\n",
        "        return completion_response_to_chat_response(completion_response)\n",
        "\n",
        "    @llm_chat_callback()\n",
        "    def stream_chat(\n",
        "        self, messages: Sequence[ChatMessage], **kwargs: Any\n",
        "    ) -> ChatResponseGen:\n",
        "        prompt = self.messages_to_prompt(messages)\n",
        "        completion_response = self.stream_complete(prompt, formatted=True, **kwargs)\n",
        "        return stream_completion_response_to_chat_response(completion_response)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def complete(\n",
        "        self, prompt: str, formatted: bool = False, **kwargs: Any\n",
        "    ) -> CompletionResponse:\n",
        "        response = self._model.submit(prompt, api_name=\"/chat\")\n",
        "\n",
        "        return CompletionResponse(text=response, raw=response)\n",
        "\n",
        "    @llm_completion_callback()\n",
        "    def stream_complete(\n",
        "        self, prompt: str, formatted: bool = False, **kwargs: Any\n",
        "    ) -> CompletionResponseGen:\n",
        "        response_iter = self._model.submit(prompt, api_name=\"/chat\")\n",
        "\n",
        "        def gen() -> CompletionResponseGen:\n",
        "            text = \"\"\n",
        "            for response in response_iter:\n",
        "                text += response\n",
        "                yield CompletionResponse(delta=response, text=text, raw=response)\n",
        "\n",
        "        return gen()"
      ],
      "metadata": {
        "id": "Q3f8LNahjnmv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}